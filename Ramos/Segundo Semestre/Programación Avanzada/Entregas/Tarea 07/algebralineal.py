# -*- coding: utf-8 -*-
"""AlgebraLineal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NQJuWRP90mioKOww0UwB6KYELeulieeb
"""

# Paralelización (Solver: Lineales o no lineales).
# Petsc

!pip install petsc4py #

# Anaconda (Jupyter-Notebook)
# Utilizar las herramientas de petsc (Localmente)
# MUMPS (Jupyter-Notebook)
# PETSC



"""# 5 noviembre (solvers con librerias en paralelo)

# Normas para una buena escritura de Python:
# https://peps.python.org/pep-0257/

# * Pandas
# * Scipy sparse (matrices dispersas)
# * Clases de funciones (Programación orientada a objeto)
# * decoradores...
# * binders (crear un archivo: CMake (c++)
# * binder con Fortran

# * Escribir un archivo archivo.sh (python)

# * Git (Github)
# * Gitlab
# * https://bitbucket.org/product/

# Solver directos optimizados por libreria scipy (matrices dispersas)
"""

import numpy as np
import scipy.linalg as spla

# Definimos una matriz

A = np.array([[3,1,6],[2,1,3],[1,1,1]])
b = np.array([9,7,3])

#factorización lu

lu_piv  = spla.lu_factor(A)

x = spla.lu_solve(lu_piv,b)
print(x)

print(lu_piv)

import numpy as np
from scipy.linalg import cho_factor, cho_solve
A = np.array([[9, 3, 1, 5], [3, 7, 5, 1], [1, 5, 9, 2], [5, 1, 2, 6]])
c, low = cho_factor(A) # realiza la descomposicion de cholesky de A y devuelve una matriz triangular inferior (y un indicador pasa saber si la parte superior o inferior que se ha almacenado)


x = cho_solve((c, low), [1, 1, 1, 1]) #cho_solve utiliza la descomposicion de Cholesky para resolver el sitema A x= b
x

# material complementario: Cuenta el numero de operaciones que se necesitan
# Cramer
# lu
# cholesky

# Métodos iterativos:
# Método de Jacobi

"""# Implementación del método de Jacobi en Scipy

Para este metodo se necesita que Diagonal no sea cero, si no hay que permutar.

$$ A x = b$$

$$ PA x = Pb $$

(Métodos precondicionador)
"""



import numpy as np
from scipy.linalg import norm

# Definir la matriz A y el vector b
A = np.array([[4, -1, 0, 0],
              [-1, 4, -1, 0],
              [0, -1, 4, -1],
              [0, 0, -1, 3]])

b = np.array([15, 10, 10, 10])

# Condiciones iniciales
x = np.zeros_like(b)  # Vector inicial (x0)

# Ax_0 = b;  x_0 = 0
# A(x_1) = b

tolerancia = 1e-10   # Tolerancia para la convergencia
max_iteraciones = 1000  # Máximo número de iteraciones

# Implementación del método de Jacobi usando scipy/numpy
def jacobi_scipy(A, b, x, tolerancia, max_iteraciones):
    D = np.diag(np.diag(A))     # Extrae la diagonal de A
    L_plus_U = A - D            # Matriz L + U

    for k in range(max_iteraciones):
        x_nuevo = (b - np.dot(L_plus_U, x)) / np.diag(A)  # Fórmula de Jacobi
        # Comprobar la convergencia
        if norm(x_nuevo - x, ord=np.inf) < tolerancia:
            print(f"Convergencia alcanzada en {k+1} iteraciones.")
            return x_nuevo
        x = x_nuevo

    print("Número máximo de iteraciones alcanzado.")
    return x

"""# Método del gradiente conjugado:
# A debe ser simétrica y definida positiva.

Minimizar este problema
$$ \phi(x)= \frac{1}{2} x^tA x -b^t x $$
"""

import numpy as np
from scipy.sparse.linalg import cg

# Definir la matriz A (simétrica y definida positiva) y el vector b
A = np.array([[4, 1], [1, 3]])
b = np.array([1, 2])

x0 = np.zeros_like(b)

# Definir la matriz A (simétrica y definida positiva) y el vector b
A = np.array([[4, 1], [1, 3]])
b = np.array([1, 2])

x = cg(A, b, x0)
x

help(cg)

"""Como resolver este problema
$$
\begin{pmatrix}
A & B \\
B^t & 0
\end{pmatrix}
\begin{pmatrix}
x_1 \\
x_2
\end{pmatrix}
=
\begin{pmatrix}
b_1 \\
b_2
\end{pmatrix}
$$

$Ax = b$

Resolver con gmres

$$ Ax_1 + Bx_2 = b_1$$
$$B^t x_1 = b_2$$

$$x_1 + A^{-1}B x_2 = A^{-1}b_1$$
$$B^tx_1 = b_2 $$

$$ B^tx_1 + B^t A^{-1}B x_2 = A^{-1}b_1 $$


$$ B^t A^{-1} B x_2 = A^{-1} b_1  - b_2$$
"""

import numpy as np
from scipy.sparse.linalg import gmres

# Definir la matriz A
A = np.array([[4, 1], [1, 3]])
b = np.array([1, 2])

x0 = np.zeros_like(b)

# Definir la matriz A (simétrica y definida positiva) y el vector b
A = np.array([[4, 1], [1, 3]]) #simetrica y definida positiva
b = np.array([1, 2])

x = gmres(A, b, x0)
x

# Solvers (No lineales)

#A no es cuadrada

# min \| Ax-b\|

